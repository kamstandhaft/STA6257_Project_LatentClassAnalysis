---
title: "Latent Class Analysis"
author: "Daniel Martin, Kameron Standhaft, Esra Ari Acar"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
    toc: true
    css: styles.css
    theme: zephyr
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
editor: 
  markdown: 
    wrap: 72
---

## 1. Introduction

Latent Class Analysis (LCA) is a statistical modeling method that can be
utilized when it is believed that there may be unobserved subgroups
(classes) among the individuals within a population. To identify these
unobserved groups, LCA searches for and analyzes response patterns that
may exist within categorical variables reported in the dataset
[@nylund2018ten] with the goal of assigning each observation into a class based on the posterior probability of certainty that an observation should belong to a particular class [@vermunt2010latent]. Latent Class Analysis is an example of a broader class
of models called finite mixture models(FFM) [@grimm2021model] that seek
to identify and classify instances of variance within model parameters
between unobserved groups within the data that have not been explicitly defined as
groups by the attributes provided within the dataset itself. Ultimately,
it is the "latent class prevalence" which is being estimated through a
process in which the LCA function is estimating the probability that
every entry being assessed from the dataset belongs within each of the
classes generated by the model[@law2016primer].

LCA specifically makes the assumption that within the population
potentially exists a finite number of unobserved subgroups, or latent
classes, and that each individual belongs to only one of these classes
[@weller2020latent]. The overarching goal of LCA is to identify where a
specific subset of classes exist within a group and estimate the
proportions of those subset classes within the overall population.
Additionally, as shown in (Figure 1.0.1), LCA can be used to help determine if any relationships exist between the identified latent classes and any other observed
variables within the dataset.

![Figure 1.0.1 - LCA 2-Class Model Representation](index_files/figure-html/lca_2class_overlap_example.jpg)


*The dotted lines show the 2 unobserved latent classes that exist within
the overall population distribution.*[@sinha2021practitioner]

As computational efficiency has improved, Latent Class Analysis has seen
an uptick in interest for researchers across a variety of fields with
applications in science, business, and health
[@petersen2019application]. LCA is often most closely associated with
identifying patterns of behavior or psychological characteristics that
prove often otherwise difficult groups to identify[@kaplan2004sage].

### 1.1 LCA Applications

One frequent application of Latent Class Analysis (LCA) has been to
utilize it to identify subgroups of individuals within a population that
may have similar or associated symptoms or characteristics of a mental
health profile [@petersen2019application] or when and for which specific
barriers may exist to receiving medical care [@thorpe2011patterns]. LCA
has also been used to identify when any subgroups of a population exist
containing individuals that may have different risk factors or potential
outcomes of a particular disease or disease pattern
[@petersen2019application]. Outside of health field applications, Latent
Class Analysis can also be used in business and marketing to assess
buying habits and even latent classes for groups with preferences for
specific types of products [@kaplan2004sage].

### 1.2 Challenges with Implementation and Analysis of Classes using LCA

LCA has the ability to use mixture-modeling to distinguish and label
instances of heterogeneity within a population and to identify subgroups
of characteristics or individuals that may be more likely to exhibit
those characteristics of heterogeneity [@weller2020latent]. Identifying
and labeling subgroups of a population that may not be explicitly
obvious based upon its reported attributes has substantial application
for helping to identify individuals within that population who may be at
greater risk or likelihood for a particular outcome. This application
can help drive execution of actions that may require intervention
[@petersen2019application].

Latent Class Analysis has application across a variety of fields, but
there are also limitations that exist with to its use under certain
circumstances that can prove challenging to its implementation. One of
the major challenges that exist in any attempted implementation of this
method, is being able to determine the appropriate number of latent
classes to choose for modeling the dataset. Doing so can prove time intensive and cumbersome, as the
process can be somewhat subjective and often requires using a multi-step
process to test multiple models and/or number of classes
[@asparouhov2014auxiliary].

Interpretation of the classes themselves that were generated can also
provide some challenge as it may require knowledge of not
only the implementation methodology of the LCA upon the data but also
requires that the analyst have access to background information about
the data attributes and context [@weller2020latent]. Though a class may
be correctly identified, if the researcher/analyst does not correctly
identify what is heterogenous about that class, it can lead to improper
naming and assumptions about the defining characteristics of the members
of that class for analysis and application [@weller2020latent].

## 2. Methods

### 2.1 Explanation of Latent Class Analysis (LCA)

Latent class analysis (LCA) is a statistical method used to identify
hidden patterns or clusters within a dataset. This technique groups a
set of observations based on several variables into distinct and
exclusive categories, where the variables are unrelated within each
category but similar across each one. Essentially, LCA as a type of
finite mixture model (FMM), also known as unsupervised learning models,
combines different distributions to group similar data based on
selected parameters, resulting in data segmentation[@naldi2020research].

In LCA, there are two types of parameters. The first type is the
inclusion probabilities, which represent the probability of a random
case in a population being assigned to any latent class. The second type
is the conditional probabilities, which describe the likelihood of a
variable taking a certain value, given that it belongs to a specific
class[@naldi2020research].

To determine the parameters of the sub-distributions in LCA, maximum
likelihood estimation is often used with the expectation-maximization
algorithm. This algorithm is a well-established method for evaluating
the goodness of fit of a statistical model. The standard equation
[@naldi2020research] for LCA is:

$$ p(x_i)=\sum_{k = 1}^{K}{p_k}\prod_{n = 1}^{N}{p_n(x_{in}|k)},$$

where $p(x_i)$ is the probability of observing a particular combination
of responses in a group of $N$ variables, $p_k$ is the probability of
membership in LC $k$, and $p_n(x_{in}|k)$ is the probability of response
to variable $n$, conditional on membership in LC $k$.

### 2.2 General Steps of Latent Class Analysis (LCA)

The general steps to use latent class analysis are as follows:

- Identify the research question and define the variables, specifically the categorical variables that will be used in the analysis

- Select the appropriate software: In this project R Studio will be used for the LCA

- Determine the number of latent classes to assess for ideal model

- Estimate the model: Estimate the model parameters using maximum likelihood estimation or Bayesian estimation

- Evaluate the model: After estimating the model, evaluate its fit using goodness-of-fit statistics such as the likelihood ratio test, Akaike Information Criterion $(AIC)$, and Bayesian Information Criterion $(BIC)$, entropy, and chi-square Goodness-of-Fit $(\chi^2)$. If the model fits well, interpret the results and report the estimated probabilities of individuals belonging to each class.

- Conduct sensitivity analysis: Finally, conduct sensitivity analyses to test the robustness of the results and evaluate the stability of the estimated probabilities across different subgroups or samples.

## 3. Analysis and Results

### 3.1 Software

We will be using the RStudio poLCA package library, which allows us to specify the number of classes for which we want RStudio to generate an LCA
model. This package provides several tools for fitting and assessing LCA
models.

**Options to be utilized:**

-   **na.rm**: allows the poLCA to function to account for any circumstances in which values may be missing within the data fields.
-   **graphs**: displays parameter estimates as a proportion from 0.0 to 1.0, for the attributes within the model
-   **nrep**: allows specification of the number of random starting values used by the poLCA function to assess the model for the best BIC value
-   **verbose**: used to assess individual attribute predictions as well
    as concisely providing fit statistics
-   **maxiter**: assigns the maximum number of iterations that can be used per run (**nrep**) to achieve convergence of the poLCA estimation algorithm


**All Necessary Libraries are loaded here:**

```{r, warning=FALSE, echo=T, message=FALSE}
#loading packages 
library(tidyverse)
library(dplyr)
library(knitr)
library(kableExtra)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(poLCA)
library(stats)
library(sjPlot)
library(ggcorrplot)
library(corrplot)
library(ggrepel)

#install.packages("FactoMineR")
#install.packages("factoextra")

library(FactoMineR)
library(factoextra)
library(ggalt)

```


### 3.2 Dataset Description

For this project we will be exploring how well Latent Class Analysis
will group our dataset of zoo animals from Kaggle called [Zoo
Animals](https://www.kaggle.com/datasets/ulrikthygepedersen/zoo-animals) into subgroups based upon unmeasured homogeneity across 14 categorical variables collected for each animal.

There are 101 entries and a total of 18 variables in this dataset. The
variables give information about the physical attributes of each of the
animals, such as if they have hair, if they have teeth, if they lay
eggs, etc. Of the 18 variables, 15 of them are binary variables and one
of them is discrete. This majority of binary variables is why we chose
this dataset for Latent Class Analysis.

As a comparison metric for assessing the quality of the LCA models generated, the variable that we are concerned with the most is the "type" variable. This variable has 7 categories: mammals, fish, birds, insects, reptiles, invertebrates, amphibians. Of the 101 entries, 42 are mammals (41.58%), 21 are birds (20.79%), 14 are fish (13.86%), 11 are invertebrates (10.89%), 8 are insects (7.92%), 6 are reptiles (5.94%), and 5 are amphibians (4.95%).

Among other variables, we will be removing the "type" variable from the
dataset. Latent Class Analysis will be utilized to ascertain whether the
groups identified by this method correspond to the seven animal types or if there is a different set of subclasses identified by the best LCA model as assessed by the convential LCA model assessment criteria.


**Table 3.2.1 Variables considered in LCA Analysis:**

```{r, warning=FALSE, echo=T, message=FALSE}

#Creating data frame of variables used and not used for displaying in tables
tab_lca_var <- data.frame(var_num = 1:14, 
   Variable_data_type = c('logi', 'logi','logi','logi','logi','logi',
               'logi','logi','logi','logi','logi','logi','logi','logi'),                       
   Variable = c('hair', 'feathers','eggs','milk','airborne','aquatic',
               'predator','toothed','backbone','breathes','venomous',
               'fins','tail','has_legs'),
   Variable_Description = c('Has hair?','has feathers?','Lays eggs?',
                           'Produces milk?','Can fly?','Can swim?',
                           'Is a predator?','Has teeth?','Has a backbone?',
                           'Animal breathes?','Has venom?','Has fins?',
                           'Has a tail?','Has legs?'))

tab_lca_var_copy <- tab_lca_var
colnames(tab_lca_var_copy) <- c("# of Variables", "Data Type", "Variables", "Variable Description")

#Printing a table of variables used in LCA modeling
kable(tab_lca_var_copy, format = "html", caption = "Variables considered in LCA Analysis") %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
    column_spec(1, bold = TRUE, color = "white", background = "steelblue") %>%
    column_spec(2:4, background = "lavender") %>%
    row_spec(0, bold = TRUE, color = "white", background = "darkblue")
```



**Table 3.2.2 Variables in Original Dataset, but not used in LCA models:** *These will be used in our assessment of LCA Model Quality*

```{r, warning=FALSE, echo=T, message=FALSE}
tab_nonlca_var <- data.frame(var_num = 1:4, 
   Variable_data_type = c('logi', 'logi','logi','logi'),
   Variable = c('animal', 'domestic','catsize','type'),
   Variable_Description = c('Animal Name','Commonly domesticated?',
                            'About the size of a cat?',
                           'Animal Classification'))

tab_nonlca_var_copy <- tab_nonlca_var
colnames(tab_nonlca_var_copy) <- c("# of Variables", "Data Type", "Variables", "Variable Description")

#Printing a table of variables not-used in LCA modeling
kable(tab_nonlca_var_copy, format = "html", caption = "Variables in Original Dataset, but not used in LCA models:") %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
    column_spec(1, bold = TRUE, color = "white", background = "steelblue") %>%
    column_spec(2:4, background = "lavender") %>%
    row_spec(0, bold = TRUE, color = "white", background = "darkblue")
```

### 3.3 Load Data

In the original raw dataset, within each categorical variable text, there is
an apostrophe and a "b" before each word and at the end, there is an
apostrophe. The data were cleaned into the file Zoo_clean with these
extra characters removed leaving only "TRUE" & "FALSE".

Below is where we import the cleaned dataset without the apostrophes or
the "b"s.

```{r, warning=FALSE, echo=TRUE}
Zoo_clean<- (read.csv("https://raw.githubusercontent.com/kamstandhaft/STA6257_Project_LatentClassAnalysis/main/Zoo_clean.csv"))

new_zoo <- Zoo_clean %>% mutate(animal=NULL, domestic=NULL, catsize=NULL, type=NULL)
view(new_zoo)

```

### 3.4 Data Preparation

As mentioned above, we will be removing the "type" variable from the
dataset with the goal of trying to replicate this level of
classification through the latent class detection. The identifier
variable "animal" will also be removed. Finally, we only want objective
binary variables in our dataset, so we will also be removing "catsize"
and "domestic" from the dataset, and transforming legs (denoting the
number of legs a creature has) to has_legs (a binary operator indicating
whether any legs are present or not).

We will be using the poLCA function from the poLCA package library,
which requires an input of positive integer values $\ge 1$. As such, we
transformed our dataset from logical operators TRUE/FALSE to binary
integers 0/1. But as poLCA function cannot accept "$0$" as a value, we
further transformed the data by incrementing all values by $1$, wherein
"FALSE" is represented by "$1$", and "TRUE" is represented by "$2$".

```{r, warning=FALSE, echo=TRUE}
#Creating "has_legs" to denote whether the animal has a leg count >0 from "legs" variable
new_zoo <- Zoo_clean %>% mutate(has_legs = ifelse(legs == 0, 0, 1))

#Removing "legs" since it is continuous and not binary
new_zoo <- new_zoo %>% mutate(legs = NULL)

#Converting has_legs from integer to logical to match the rest of the dataset
new_zoo$has_legs <- as.logical(new_zoo$has_legs)

#Removing "animal","domestic", "catsize", and "type" from 18-variable dataset to make new 14-variable dataset for LCA modeling  
new_zoo_subset <- new_zoo %>% mutate(animal=NULL, domestic=NULL, 
                                     catsize=NULL, type=NULL)

#Creating a dataset where the binary data is converted to integers 0/"FALSE" or 1/"TRUE"
new_zoo_int <- new_zoo_subset %>% mutate_all(~as.integer(.))

#Incrementing the binary integers from 0/1 to 1/2 - poLCA() can only accept integers >=1
new_zoo_int_1 <- new_zoo_int %>% mutate_all(~. + 1)
```

## 4. Statistical Modeling

Potentially the most difficult decision to make regarding identification of latent classes is determining the correct number of classes to use in our model. We will be assessing 2,3,4,5,6, and 7 number of classes in modeling. 

Below we begin preparing to build our models by using the cbind function used to bind Columns being used in LCA.

```{r, warning=FALSE, echo=TRUE}
# cbind function used to bind Columns being used in LCA
lca_bind <-  cbind(hair, feathers, eggs, milk, 
                   airborne, aquatic, predator, 
                   toothed, backbone, breathes, 
                   venomous, fins, tail, has_legs) ~ 1
```

### 4.1 Fitting LCA Models with 2-7 \# of Classes

We will be running each of our 6 models 100 times(100 random starting values) each with 100 iterations. This will allow us to have consistent results for each number of classes per model to report upon without large swings in class sizes and predictions, and will additionally provide a best model for each number of classes.

**Fitting a 2-class LCA Model**

For the 2-class model, we have selected to display the output of the
poLCA model results using the \[graph = TRUE\] option.

For each variable, this indicates the percentage of members of the
original dataset that have that attribute broken down by which latent
class was assigned in the model for that number of classes.

We chose to assess up to 7-class models because the original \# of
classifications was 7 "types".

```{r, warning=FALSE, echo=TRUE}
lca_fit2 <- poLCA(lca_bind, data = new_zoo_int_1, 
                  nclass = 2, graphs = FALSE, na.rm = TRUE, 
                  nrep=100, maxiter=100, verbose = FALSE)
```

![Figure 4.1.1 - LCA 2-Class Model](index_files/figure-html/2_class_model.png)

(Figure 4.1.1) shows the graphical output of the poLCA model when
2-Classes are specified. The proportions at the base of the plot (0.3953
& 0.6047) indicate the percentage of the original dataset that the poLCA
function has classified into each of the detected latent classes.

The vertical red bars \[Pr(outcome)\] denote the percentage of the
members of that assigned class have that particular attribute. While the
proportions at the base of the plot should always add to 1.0, the member
percentage of each assigned class may have any value from 0-1.0 across
all classes for that attribute. The poLCA function will additionally output a proportion value  between 0 and 1 denoting the likelihood that each attribute will be included within each class generated by the function, as seen in (Figure 4.1.2) below.

![Figure 4.1.2 - LCA 2-Class Proportion Output](index_files/figure-html/2_class_prop_output.png)

**Fitting a 3-class LCA Model**

```{r, warning=FALSE, echo=TRUE}
lca_fit3 <- poLCA(lca_bind, data = new_zoo_int_1, 
                  nclass = 3, graphs = FALSE, na.rm = TRUE, 
                  verbose = FALSE, nrep=100, maxiter=100)
```

**Fitting a 4-class LCA Model**

```{r, warning=FALSE, echo=TRUE}
lca_fit4 <- poLCA(lca_bind, data = new_zoo_int_1, 
                  nclass = 4, graphs = FALSE, na.rm = TRUE, 
                  verbose = FALSE, nrep=100, maxiter=100)
```

**Fitting a 5-class LCA Model**

```{r, warning=FALSE, echo=TRUE}
lca_fit5 <- poLCA(lca_bind, data = new_zoo_int_1, 
                  nclass = 5, graphs = FALSE, na.rm = TRUE,
                  verbose = FALSE, nrep=100, maxiter=100)
```

**Fitting a 6-class LCA Model**

```{r, warning=FALSE, echo=TRUE}
lca_fit6 <- poLCA(lca_bind, data = new_zoo_int_1, 
                  nclass = 6, graphs = FALSE, na.rm = TRUE,
                  verbose = FALSE, nrep=100, maxiter=100)
```

**Fitting a 7-class LCA Model**

```{r, warning=FALSE, echo=TRUE}
lca_fit7 <- poLCA(lca_bind, data = new_zoo_int_1, 
                  nclass = 7, graphs = FALSE, na.rm = TRUE,
                  verbose = FALSE, nrep=100, maxiter=100)
```

### 4.2 Model Comparison Statistics

As determining the correct number of classes to use to properly model the data can often prove challenging, we will use a number of statistics to help indicate fit quality for each model, which we go through below to help select the correct number of classes for a good model.

**Note**: For each of these 6 models, we are running 100 repetitions random starting value, each with 100 iterations to help achieve convergence of the models for comparison. The criteria for which the best model is chosen by the poLCA function is the lowest BIC value. Without increasing the number of runs and iterations, the model and fit statistics can be inconsistent.

To compare these models, we will be primarily looking at the Akaike
Information Criterion $(AIC)$, the Bayesian Information Criterion $(BIC)$,
and the Chi-square goodness of fit statistic $(\chi^2)$.

In a traditional research environment, these statistics would be likely
the best decision criteria we could expect to have for assessing the
model. In the case of our data, we have a comparison classification
attribute "type" in our dataset that we will withhold from the Latent
Class Analysis procedure as a secondary means to assess how well the
model classifies the members of the dataset - in this case animals
within a zoo. It is possible, however, that the LCA may find latent
classes that differ from these defined classifications, which roughly
correspond to the Linnaean taxonomic classification of classes within
the Kingdom Animalia.

A lower value for the AIC and BIC values as well as the $(\chi^2)$, indicate
that the model is a better fit for the data.

As illustrated in the figure below,the 5-class, 6-class, and 7-class
models all have substantially lower values for these statistical metrics
than for the 2-class, 3-class, and 4-class models. It is worth noting
however, that the 7-class model posts an alert indicating that the
number of parameters estimated exceeds the number of observations, which
effectively provides a negative residual Degree of Freedom (DoF). For
this reason, we can eliminate the 7-class model.

**Table 4.2.1 Comparison of AIC, BIC, and Goodness of Fit for each # of Classes**
```{r, warning=FALSE, echo=TRUE}
# Load Data
#![Figure 4.2.1 Model Comparison](index_files/figure-html/Model_comparison.PNG)

class_count <- c(2, 3, 4, 5, 6, 7)

fit_stats_long <- data.frame(
  class_count = rep(class_count, 2),
  fit_stat =       c(rep("AIC", length(class_count)), rep("BIC", length(class_count))),
  fit_stat_value = c(lca_fit2$aic, lca_fit3$aic, lca_fit4$aic, lca_fit5$aic, lca_fit6$aic, lca_fit7$aic,
                   lca_fit2$bic, lca_fit3$bic, lca_fit4$bic, lca_fit5$bic, lca_fit6$bic, lca_fit7$bic))

fit_AIC <- c(lca_fit2$aic, lca_fit3$aic, lca_fit4$aic, lca_fit5$aic, lca_fit6$aic, lca_fit7$aic)
fit_BIC <- c(lca_fit2$bic, lca_fit3$bic, lca_fit4$bic, lca_fit5$bic, lca_fit6$bic, lca_fit7$bic)
fit_GoF <- c(lca_fit2$Chisq, lca_fit3$Chisq, lca_fit4$Chisq, lca_fit5$Chisq, lca_fit6$Chisq, lca_fit7$Chisq)

fit_stats <- data.frame(class_count, fit_AIC, fit_BIC, fit_GoF)

#Making a copy with easier to read column names
fit_stats_copy <- fit_stats
colnames(fit_stats_copy) <- c("Class Count", "AIC", "BIC", "Goodness of Fit")

#Using the knitr library

kable(fit_stats_copy, format = "html", caption = "Fit Statistics Table") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE, color = "white", background = "steelblue") %>%
  column_spec(2:4, background = "lavender") %>%
  row_spec(0, bold = TRUE, color = "white", background = "darkblue")
```

The 5-class model has a better fit according to the BIC(1177) values
compared to the 6-class model BIC(1199), but the AIC for the 6-class
model (966) is slightly better than the 5-class model AIC(983).

```{r, warning=FALSE, echo=TRUE}

ggplot(data = fit_stats_long, aes(x = class_count, y = fit_stat_value, color = fit_stat)) +
  geom_line() +
  geom_point(color='black') +
  labs(
    title = "AIC and BIC Comparison",
    x = "Number of Classes",
    y = "Fit Statistics Value",
    color = "Fit Statistics"
  ) +
  scale_color_discrete(labels = c("AIC", "BIC")) +
  theme(plot.title = element_text(hjust = 0.5)) 
# + facet_wrap(~ fit_stat, ncol = 1)

```
**Figure 4.2.1 - Comparison of AIC and BIC across 2, 3, 4, 5, 6, and 7-Class Models**

We additionally look at the Chi-square goodness of fit statistic $(\chi^2)$.
The GoF $(\chi^2)$ for the 5-class model(1858) is better however than the
$(\chi^2)$ for the 6-class model(2283).

```{r, warning=FALSE, echo=TRUE}
# Goodness of Fit

fit_GoF_stat <- data.frame(class_count, fit_GoF)

ggplot(data = fit_GoF_stat, aes(x = class_count, y = fit_GoF)) +
  geom_line(color = 'green') +
  geom_point() +
  labs(
    title = "Goodness of Fit",
    x = "Number of Classes",
    y = "Goodness of Fit Statistic Value") +
  theme(plot.title = element_text(hjust = 0.5))
```
**Figure 4.2.2 - Comparison of GoF Values across 2, 3, 4, 5, 6, and 7-Class Models**

Without a clearly better model between these two, we can further compare
their entropy (see below).

The entropy of a model speaks to how well the classes are separated with
a higher entropy value indicating a likely increase in the separation of
classes.

As we see below, the 5-class model has a slightly higher entropy value
than the 6-class model implying a slightly higher separability of
classes. With BIC, GoF, and entropy in favor of the 5-class model, we
select this model to use to assess and illustrate the classification
schema that LCA uses to classify the dataset.

**Table 4.2.3 Comparison of Entropy Values for the 5-Class and 6-Class Models**
```{r, warning=FALSE, echo=TRUE}

lca_fit5.ent <- poLCA.entropy(lca_fit5)
lca_fit6.ent <- poLCA.entropy(lca_fit6)

tab_lca_entropy <- data.frame( 
   Model_Class_Number = c('5-Class Model', '6-Class Model'),
   Entropy = c(lca_fit5.ent,lca_fit6.ent))

tab_lca_entropy_copy <- tab_lca_entropy
colnames(tab_lca_entropy_copy) <- c("Model Classes", "Entropy Value")

#Using the knitr library

kable(tab_lca_entropy_copy, format = "html", caption = "Entropy Comparison of 5-Class and 6-Class Models") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE, color = "white", background = "steelblue") %>%
  column_spec(2, background = "lavender") %>%
  row_spec(0, bold = TRUE, color = "white", background = "darkblue")

```

### 4.3 Chosen Model Discussion and Assessment

With the better AIC, GoF $(\chi^2)$, and Entropy value, we selected the
5-Class model as our preferred model.

The 5-class population proportion graph shown below (Figure 4.3.1) displays along the x-axis, the proportions of the total dataset population that were classified into each of the 5-class of these model.

![Figure 4.3.1 - 5-Class Model](index_files/figure-html/5_class_ordered.png)

Under a normal research environment, we would not likely have the true
classifications to compare these classes to against their predicted
counterparts. But as we chose a dataset for which we could remove
classification values and have the LCA models assess the attributes of
the members, we designed the experiment to illustrate how Latent Class
Analysis predicts which class a member should belong to, and can compare
these to against a real classification schema.

For the purposes of these figures, we filtered selections based on
whether the attributes had above \~70% likelihood of being members in
the class according to the model. By doing so, there is the potential
for overlap with certain member showing in more than one class, but with
that in mind, this approach has the benefit of showing how potentially
effective LCA can be at detecting these latent groups.

**Table 4.3.1 Comparison of AIC, BIC, and Goodness of Fit for each # of Classes**
```{r, warning=FALSE, echo=TRUE}

#Reordering the Model output graph to display in order from highest to lowest proportion for easier comparison and labeling
probs.start.new <- poLCA.reorder(lca_fit5$probs.start,order(lca_fit5$P,decreasing=TRUE))

lca_fit5 <- poLCA(lca_bind, data = new_zoo_int_1,nclass=5,
                  graphs = FALSE,na.rm = TRUE,
                  verbose = FALSE, nrep=100, maxiter=100,
                  probs.start=probs.start.new)


orig_classes <- data.frame( 
   Class = c('Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'),
   Percentage = c('35.46%', '20.78%','17.82%','13.09%','12.85%'))

kable(orig_classes, format = "html", caption = "Population Percentages of each Class in the 5-Class Model") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE, color = "white", background = "steelblue") %>%
  column_spec(2, background = "lavender") %>%
  row_spec(0, bold = TRUE, color = "white", background = "darkblue")
```

**Class 1** which, as the largest class, is estimated to contain
\~35.46% of the population predicts its members to have 'hair', 'toothed',
'backbone', 'breathes', 'tail', 'has_legs'. We filtered the cleaned new_zoo
dataset for these attributes, and noted that all members were classified
as type 'mammal' in the original dataset.

**Class 2** which is estimated to contain \~20.78% of the population
predicts its members to have 'feathers', 'eggs', 'backbone', 'breathes', 'tail',
'has_legs'. We filtered the cleaned new_zoo dataset for these attributes,
and noted that all members were classified as type 'bird' in the
original dataset.

**Class 3** is estimated to contain ~17.82% of the population. It should be noted that it has two
attributes ('breathes' and 'predator') which were both over 50% shown on the
graph, but as we excluded our initial filtering assessment to \~80% and above, these are
not selected in the filter.

These class members are predicted to have 'eggs' and 'has_legs'. While this
limited selection duplicates a few of the previous members that were
selected for other classes, this class also contains the invertebrates
and insects, which were not present in the other classes. Based solely upon outside comparison to the type attribute in the original dataset, this class appears to be the least homogenous.

**Class 4** which is estimated to contain \~13.09% of the population is
less homogenous than Classes 1 & 2, but not as heterogenous as Class 3.
The original classification schema of "type" had 7 unique values, which
implies that with a 5-class model, overlap is inevitable. This is only
visible due to the fact that we have the original data for "type" for
illustration purposes, which is unlikely to be the case in research
data.This class' members ar predicted to have 'aquatic', 'predator',
'toothed', 'backbone', 'breathes', 'tail'. We filtered the cleaned new_zoo
dataset for these attributes. The resulting filter only yielded 5
members, which are mostly aquatic mammals and a single amphibian.

**Class 5** which is the smallest class, estimated to contain \~12.85%
of the population predicts its members to have 'eggs', 'aquatic', 'toothed',
'backbone', 'fins', 'tail'. We filtered the cleaned new_zoo dataset for these
attributes, and noted that all members were classified as type 'fish' in
the original dataset.

```{r, warning=FALSE, echo=TRUE}

#Here we are converting the new_zoo_int_2 into a factor so it can be used with MCA() function
new_zoo_int_1_factor <- as.data.frame(lapply(new_zoo_int_1, factor))

#Saving the result of MCA function applied to new_zoo_int_2_factor into mca_outcome
#Made invisible so the output does not print instead of image with label
mca_outcome <- invisible(MCA(new_zoo_int_1_factor, graph = FALSE))

#Setting the # of dimension in the biplot to 2 (just x and y axis)
n_dimensions <- 2

#Creating the MCA biplot from mca_outcome
mca_biplot <- fviz_mca_biplot(
  mca_outcome,
  repel = TRUE,
  axes = c(1, 2), # we are only displaying axes 1(Dim1)& 2(Dim2)
  title = paste("MCA Biplot (Dimensions:", n_dimensions, ")"))

#Printing the MCA biplot
#mca_biplot

#----

#Extract the class membership probabilities and convert to dataframe
class_probability <- lca_fit5$posterior
class_probability_df <- as.data.frame(class_probability)

#Build a matrix from the dataframe and assign each observation to the LCA class with the highest probability
class_probability_matrix <- as.matrix(class_probability_df)
lca_result_classes <- max.col(class_probability_matrix, ties.method = "random")

#Turn lca_result_classes into a factor for MCA input
lca_result_class_factor <- factor(lca_result_classes, levels = 1:5, labels = c("Class 1", "Class 2", "Class 3", "Class 4", "Class 5"))

#Find the coordinate points generated by the Multiple Correspondence Analysis (MCA) function
coord_pts <- as.data.frame(mca_outcome$ind$coord)

#Adding the LCA class labels to the dataframe with the coordinates
coord_pts$lca_result_class_factor <- lca_result_class_factor

#Plot the LCA classes as ellipses over the MCA plot of plots and save as LCA_MCA_biplot
LCA_MCA_biplot <-
  ggplot(coord_pts, aes(x = `Dim 1`, y = `Dim 2`, color = lca_result_class_factor)) +
  geom_point(size = 3, alpha = 0.8) +
  stat_ellipse(aes(x = `Dim 1`, y = `Dim 2`, color = lca_result_class_factor), type = "norm", level = 0.95) +
  geom_text(aes(label = rownames(coord_pts)), nudge_x = 0.02, nudge_y = 0.02, size = 3, check_overlap = TRUE, color = "black") +
  labs(
    title = "MCA Biplot with LCA Classes",
    x = "Dimension 1 (31.2% Variance Explained by this Dimension)",
    y = "Dimension 2 (25.3% Variance Explained by this Dimension)",
    color = "LCA Class")

#LCA_MCA_biplot

```

![Figure 4.3.2 - MCA Biplot with 5 Class LCA Model Classes Overlain](index_files/figure-html/mca_biplot_lca.png)


### 4.4 Sensitivity Analysis - Correlation

Researchers have the ability to modify the model specification by either adding or removing variables or constraints to assess the sensitivity of the results to these changes. One of the major assumptions when performing Latent Class Analysis (LCA) is is an independence of the variables within a given class detected; variable redundancy and multicollinearity therefore becomes an important factor to consider, particularly when correlation coefficients for specific variables are greater than 0.50 [@sinha2021practitioner]. To analyse and assess the extent of this risk, we generated a correlation matrix and heatmap to assess the presence of multicollinearity, with a focus particularly on positively correlated variables that are present within the same classes, since by virtue of the underlying function of LCA, negatively correlated variables are less likely to appear within the same class.

We then test the model by removing these variables and comparing the new model to the previous model which includes these variables. We will evaluate whether the new model is still able to fit the data well and compare its goodness of fit statistics (such as AIC, BIC) to those of the previous model. If the new model shows improvement in goodness of fit, we may consider selecting it as our final model.

![Figure 4.4.1 - Correlation heatmap for sensitivity analysis](index_files/figure-html/Heatmap.png)

As seen in the correlation heatmap (Figure 4.4.1), milk is highly correlated with both hair(0.88)and egg(-0.94). We decided to remove milk as an attribute, and re-consider our LCA model to see if it improves without this highly collinear variables included.

```{r, warning=FALSE, echo=TRUE}

#Creating a dataset without 'milk' - 13-variables
new_zoo_int_2 <- new_zoo_int_1  %>% mutate(milk=NULL)

#Binding 13-variables into columns for modified LCA model without 'milk'
lca_bind_corr <-  cbind(hair, feathers, eggs, 
                   airborne, aquatic, predator, 
                   toothed, backbone, breathes, 
                   venomous, fins, tail, has_legs) ~ 1

lca_fit5_corr <- poLCA(lca_bind_corr, data = new_zoo_int_2, 
                  nclass = 5, graphs = FALSE, na.rm = TRUE,
                  verbose = FALSE, nrep=100, maxiter=100)

#Reordering the Model output graph to display in order from highest to lowest proprtion for easier comparison and labeling
probs.start.new2 <- poLCA.reorder(lca_fit5_corr$probs.start,order(lca_fit5_corr$P,decreasing=TRUE))

lca_fit5_corr <- poLCA(lca_bind_corr, data = new_zoo_int_2,nclass=5,
                  graphs = FALSE,na.rm = TRUE,
                  verbose = FALSE, nrep=100,
                  probs.start=probs.start.new2)

```

![Figure 4.4.2 - 5-Class LCA Model Population Percentages without 'milk'](index_files/figure-html/lca_fit5_corr_graph.png)


With the 'milk' variable removed from our LCA models due to high correlation values with 2 other attributes, our 5-Class models exhibited substantial improvement in AIC, BIC, and GoF, and slightly worse values for entropy. As a result, we find the adjusted 5-class model, with 13 attributes assessed to be our best final model.

**Table 4.4.1 Comparison Statistics for Original and Modified 5-Class Models**
```{r, warning=FALSE, echo=TRUE}
lca_fit5_corr.ent <- poLCA.entropy(lca_fit5_corr)

#Creating data frame of AIC, BIC, GoF, and Entropy of both original and modified 5-Class Models
class5_compare <- data.frame( 
   Model = c('Original 5-Class Model', 'Adjusted 5-Class Model'),
   AIC = c(lca_fit5$aic, lca_fit5_corr$aic),
   BIC = c(lca_fit5$bic, lca_fit5_corr$bic),
   GoF = c(lca_fit5$Chisq, lca_fit5_corr$Chisq),
   entropy = c(lca_fit5.ent, lca_fit5_corr.ent))

#Printing table comparing original and modified 5-class stats
kable(class5_compare, format = "html", caption = "Comparison Statistics for Original and Modified 5-Class Models") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(1, bold = TRUE, color = "white", background = "steelblue") %>%
  column_spec(2, background = "lavender") %>%
  row_spec(0, bold = TRUE, color = "white", background = "darkblue")

```


### Conclusion

Our research indicates that Latent Class Analysis (LCA) is an
exceptionally effective tool at classifying categorical variables based
upon their attributes. With the dataset-defined "type" classification as
a reference, our 5-class predictive model has shown that LCA as a
process can be used to successfully define where relevant classes exist
when not otherwise defined from collected data, even with our relatively
small dataset (101 points). LCA does have limited scope in that both the
indicator(categorical dependent variables) and latent variables must be
categorical, where other separate processes such as Latent Profile
Analysis (LPA), Latent Trait Analysis (LTA), and Factor Analysis exist
for circumstances when one or both of the indicator and latent variables
are continuous [@law2016primer].

With larger datasets, and proper screening of models, LCA is a powerful
tool for finding group commonalities between members of a group,
particularly in disciplines where research is often done through
self-reporting and assessment of softer-science categories such as
opinion, behavioral observations, and medical research[@kaplan2004sage].
Even highly complex data can potentially become much more meaningful
through the use of Latent Class Analysis, but the substantial challenges
that exist with determining the fit of the ideal model require
consistent acknowledgment of the possibility for error on the part of
researcher, and transparency with regard to the processes used within
the research[@law2016primer].
